import json
import time
from confluent_kafka import Consumer, Producer, KafkaError

KAFKA_BOOTSTRAP_SERVERS = "kafka:9092"
GROUP_ID = "tratador_limpeza_group"
SOURCE_TOPIC = "raw_secretary"
DEST_TOPIC = "clean_secretary"

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.types import StructType, StructField, IntegerType

# Supondo que j√° tenha a sess√£o spark criada globalmente
spark = SparkSession.builder.appName("tratador_limpeza").getOrCreate()
spark.sparkContext.setLogLevel("ERROR")

schema = StructType([
    StructField("Diagnostico", IntegerType(), True),
    StructField("Populacao", IntegerType(), True)
])

def limpeza(df):
    df_limpo = df.filter(
        (col("Diagnostico").isin(0, 1)) &
        (col("Populacao").isNotNull()) &
        (col("Populacao") > 0)
    )
    return df_limpo

def main():
    print("\n" + "="*50)
    print(" INICIANDO TRATADOR DE LIMPEZA DE DADOS ")
    print("="*50)
    print(f"Conectando ao Kafka em: {KAFKA_BOOTSTRAP_SERVERS}")
    print(f"T√≥pico de origem: {SOURCE_TOPIC}")
    print(f"T√≥pico de destino: {DEST_TOPIC}")
    print(f"Grupo de consumidores: {GROUP_ID}")
    print("="*50 + "\n")

    # Configura√ß√£o do consumidor
    consumer_config = {
        "bootstrap.servers": KAFKA_BOOTSTRAP_SERVERS,
        "group.id": GROUP_ID,
        "auto.offset.reset": "earliest",
        "enable.auto.commit": False
    }

    consumer = Consumer(consumer_config)
    producer = Producer({"bootstrap.servers": KAFKA_BOOTSTRAP_SERVERS})

    def delivery_report(err, msg):
        """Callback para confirma√ß√£o de entrega"""
        if err is not None:
            print(f"‚ùå Falha ao enviar mensagem: {err}")
        else:
            pass
            # print(f"üì§ Mensagem enviada com sucesso para {msg.topic()} [parti√ß√£o {msg.partition()}]")

    consumer.subscribe([SOURCE_TOPIC])
    print(f"üîç Inscrito no t√≥pico {SOURCE_TOPIC}. Aguardando mensagens...")

    try:
        msg_count = 0
        start_time = time.time()
        
        while True:
            msg = consumer.poll(timeout=1.0)
            
            if msg is None:
                continue
                
            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    # print("‚ÑπÔ∏è Fim da parti√ß√£o alcan√ßado")
                    continue
                else:
                    # print(f"‚ùå Erro no consumidor Kafka: {msg.error()}")
                    continue

            msg_count += 1
            # print(f"\nüì• Mensagem #{msg_count} recebida [t√≥pico: {msg.topic()}, parti√ß√£o: {msg.partition()}, offset: {msg.offset()}]")
            
            try:
                # Processamento da mensagem
                dado = json.loads(msg.value().decode('utf-8'))
                # print(f"üìù Conte√∫do bruto: {dado}")
                
                # Limpeza dos dados
                # Cria um DataFrame Spark com o dado (uma √∫nica linha)
                df = spark.createDataFrame([dado], schema=schema)

                # Aplica o filtro
                df_limpo = limpeza(df)

                # Coleta resultado em lista de dicts
                resultado = df_limpo.collect()

                if resultado:
                    # Se passou no filtro, pega a linha convertendo para dict
                    dado_limpo = resultado[0].asDict()
                    # Envia para Kafka
                    producer.produce(
                        DEST_TOPIC,
                        json.dumps(dado_limpo).encode('utf-8'),
                        callback=delivery_report
                    )
                    producer.flush()
                else:
                    print("üóëÔ∏è Registro descartado durante a limpeza")
                                
                # if dado_limpo:
                #     # Envio para o t√≥pico de sa√≠da
                #     producer.produce(
                #         DEST_TOPIC,
                #         json.dumps(dado_limpo).encode('utf-8'),
                #         callback=delivery_report
                #     )
                #     producer.flush()
                #     # print(f"üîÑ Dado limpo: {dado_limpo}")
                # else:
                #     print("üóëÔ∏è Registro descartado durante a limpeza")
                    
                # Commit do offset
                consumer.commit(asynchronous=False)
                # print(f"‚úîÔ∏è Offset {msg.offset()} confirmado")
                
            except json.JSONDecodeError as e:
                print(f"‚ùå Erro ao decodificar JSON: {e}")
            except Exception as e:
                print(f"‚ùå Erro inesperado: {e}")

    except KeyboardInterrupt:
        print("\n" + "="*50)
        print(" INTERRUP√á√ÉO SOLICITADA - ENCERRANDO CONSUMER ")
        print(f"Total de mensagens processadas: {msg_count}")
        print(f"Tempo de execu√ß√£o: {time.time() - start_time:.2f} segundos")
        print("="*50)
    finally:
        consumer.close()
        print("‚úÖ Conex√£o com Kafka encerrada corretamente")

if __name__ == "__main__":
    main()